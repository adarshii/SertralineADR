# ğŸ§¬ AI-Driven Adverse Drug Reaction (ADR) Prediction System

An explainable, interactive clinical decision-support system for predicting
**adverse drug reaction (ADR) risk** associated with **Sertraline**, integrating
clinical, pharmacological, and multi-omics features using machine learning.

This system is designed for **research, educational, and pharmacovigilance use**
and provides transparent risk interpretation through visual analytics and an
AI-assisted explanation interface.

---

## ğŸš€ Key Features

- **Machine-learningâ€“based ADR risk prediction**
- **ADR signal strength (%)** with risk categorization
- **Model confidence estimation** (uncertainty awareness)
- **Organ-specific risk visualization** (radar plots)
- **Time-dependent ADR risk timeline**
- **SHAP-based explainability** for feature contributions
- **Interactive AI assistant** for clinicians & researchers
- **â€œExplain this graphâ€** buttons with visual highlighting
- **Hover-based contextual explanations**
- **User feedback scoring** for explanation usefulness
- **Streamlit Cloud deployment ready**

---

## ğŸ–¥ï¸ Live Demo

ğŸ‘‰ *(Add your Streamlit Cloud URL here once deployed)*  
Example:  
`https://your-app-name.streamlit.app`

---

## ğŸ§  Intended Users

- **Clinicians** â€“ to interpret ADR risk and uncertainty  
- **Researchers** â€“ to explore model behavior and biological drivers  
- **Pharmacovigilance analysts** â€“ to support post-marketing safety analysis  

---

## ğŸ“Š Methodology Overview

### ğŸ”¹ Model
- **Algorithm:** LightGBM classifier  
- **Output:** ADR signal score (probability-based)  
- **Risk interpretation:** Low / Moderate / High  

### ğŸ”¹ Input Features
- Clinical factors (age, dose, comorbidities)
- Pharmacological variables
- Multi-omics features (genomics / proteomics / pathway proxies)

### ğŸ”¹ Explainability
- **SHAP values** for global and local feature importance
- Natural-language interpretation via AI assistant

### ğŸ”¹ Confidence Estimation
- Computed as distance from the model decision boundary (0.5)
- Expressed as **High / Moderate / Low confidence**

---

## ğŸ¤– AI Clinical & Research Assistant

The integrated AI assistant provides **interpretive support**, not medical advice.

It can explain:
- What the prediction means
- How confident the model is
- Why certain features increased risk
- How to interpret SHAP plots
- What each visualization represents

The assistant is **context-aware**, responding differently depending on:
- User role (Clinician vs Researcher)
- Active visualization or tab
- Model outputs for the current patient profile

---

## ğŸ“ Project Structure
Sertraline/
â”œâ”€â”€ app.py # Main Streamlit application
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ styles.css # Custom UI styling
â”œâ”€â”€ assets/ # Images, logos, screenshots
â”‚ â””â”€â”€ logo.png
â”œâ”€â”€ models/ # Trained ML models
â”‚ â””â”€â”€ model.pkl
â”œâ”€â”€ utils/ # Helper modules (optional refactor)
â”‚ â”œâ”€â”€ model_utils.py
â”‚ â”œâ”€â”€ confidence_utils.py
â”‚ â”œâ”€â”€ explain_utils.py
â”‚ â”œâ”€â”€ plot_utils.py
â”‚ â””â”€â”€ db_utils.py
â”œâ”€â”€ data/ # Reference data (if any)
â””â”€â”€ README.md # Project documentation


## âš™ï¸ Installation & Local Run

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name

